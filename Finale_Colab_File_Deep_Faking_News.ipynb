{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DeepFaking the News with NLP and Transformer Models",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Waxpple/fakenews/blob/master/Finale_Colab_File_Deep_Faking_News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kGiS6nLLjOVc"
      },
      "source": [
        "REF:\n",
        "# link:\"\"\"DeepFaking the News with NLP and Transformer Models fron Alan Geitger\n",
        "#Original file is located at\n",
        "#    https://colab.research.google.com/drive/#1VI3oBIOQYsym2x5oOux7DTNhpdR0r4uw\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzgvpvrAsDc3",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_rK3jeYNcPj",
        "colab_type": "code",
        "outputId": "b4a7e193-ab2e-440e-c644-6756bd2e3581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install python-wordpress-xmlrpc"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-wordpress-xmlrpc\n",
            "  Downloading https://files.pythonhosted.org/packages/47/a6/87d2c927e8b76ca75be7f295baf24cbba4bd3b1776d0eb88147ee81c9623/python-wordpress-xmlrpc-2.3.zip\n",
            "Building wheels for collected packages: python-wordpress-xmlrpc\n",
            "  Building wheel for python-wordpress-xmlrpc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-wordpress-xmlrpc: filename=python_wordpress_xmlrpc-2.3-cp36-none-any.whl size=16363 sha256=1263cc5546482206334dc1d380905c35ffeaa67ce881b4bb4079708e057de0a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/6a/e1/765a3b784c2a77f9c8bdaacc4a8f87ca6d54767c5c08ba779e\n",
            "Successfully built python-wordpress-xmlrpc\n",
            "Installing collected packages: python-wordpress-xmlrpc\n",
            "Successfully installed python-wordpress-xmlrpc-2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H2c5j44Namr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordpress_xmlrpc import Client, WordPressPost\n",
        "from wordpress_xmlrpc.methods.posts import NewPost\n",
        "\n",
        "#authenticate\n",
        "wp_url = \"####\"\n",
        "wp_username = \"#####\"\n",
        "wp_password = \"@#####\"\n",
        "\n",
        "wp = Client(wp_url, wp_username, wp_password)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brqmJquupaCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fake person who will be slandered/libeled in the fake articles\n",
        "NAME_TO_SLANDER = \"Peter Godamn\"\n",
        "IMAGE_TO_SLANDER = \"https://thispersondoesnotexist.com/\"\n",
        "\n",
        "SLANDEROUS_SEED_HEADLINES = [\n",
        "  f\"{NAME_TO_SLANDER} convicted of stealing puppies\",\n",
        "  f\"{NAME_TO_SLANDER} caught lying about growing the world's largest watermelon\",\n",
        "  f\"{NAME_TO_SLANDER} accused of stealing priceless artifacts from Egypt\",\n",
        "  f\"{NAME_TO_SLANDER} forged priceless works of modern art for decades\",\n",
        "  f\"{NAME_TO_SLANDER} claimed to be Pokemon master, but caught in a lie\",\n",
        "  f\"{NAME_TO_SLANDER} bought fake twitter followers to pretend to be a celebrity\",\n",
        "  f\"{NAME_TO_SLANDER} caught in the act robbing a pet store\",\n",
        "  f\"{NAME_TO_SLANDER} revealed as a foriegn spy for the undersea city of Atlantis\",\n",
        "  f\"{NAME_TO_SLANDER} involved in blackmail scandal with King Trident of Atlantis\",\n",
        "  f\"{NAME_TO_SLANDER} hid past crimes to get elected as Mayor of Otter Town\",\n",
        "  f\"{NAME_TO_SLANDER} lied on tax returns to cover up past life as a Ninja Turtle\",\n",
        "  f\"{NAME_TO_SLANDER} stole billions from investors in a new pet store\",\n",
        "  f\"{NAME_TO_SLANDER} claims to be a Ninja Turtle but was actually lying\",\n",
        "  f\"{NAME_TO_SLANDER} likely to be sentenced to 20 years in jail for chasing a cat into a tree\",\n",
        "  f\"{NAME_TO_SLANDER} recieves record prison sentence for offensive smell\",\n",
        "  f\"{NAME_TO_SLANDER} commits a multitude of crimes against dinosaurs\",\n",
        "]\n",
        "\n",
        "# Which news website to 'clone'\n",
        "#DOMAIN_STYLE_TO_COPY = \"www.nytimes.com\"\n",
        "#RSS_FEEDS_OF_REAL_STORIES_TO_EMULATE = [\n",
        "#  \"https://rss.nytimes.com/services/xml/rss/nyt/US.xml\",\n",
        "#]\n",
        "#http://feeds.bbci.co.uk/news/business/rss.xml\n",
        "DOMAIN_STYLE_TO_COPY = \"www.bbc.co.uk\"\n",
        "RSS_FEEDS_OF_REAL_STORIES_TO_EMULATE = [\n",
        "  \"http://feeds.bbci.co.uk/news/business/rss.xml\",\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pEenfAczd4Y",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Download Grover code and install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDKbiVNRJSWf",
        "colab_type": "code",
        "outputId": "052f71cd-4dca-4d9c-dce4-421e1352bce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/rowanz/grover.git\n",
        "%cd /content/grover\n",
        "!python3 -m pip install regex jsonlines twitter-text-python feedparser"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'grover'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 90 (delta 3), reused 0 (delta 0), pack-reused 84\u001b[K\n",
            "Unpacking objects: 100% (90/90), done.\n",
            "/content/grover\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.9)\n",
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Collecting twitter-text-python\n",
            "  Downloading https://files.pythonhosted.org/packages/29/a9/3d9cc947dea07e42f55a3c9de741ceeea766f841bc08297605a6370dfca0/twitter-text-python-1.1.1.tar.gz\n",
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
            "Building wheels for collected packages: twitter-text-python, feedparser\n",
            "  Building wheel for twitter-text-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twitter-text-python: filename=twitter_text_python-1.1.1-cp36-none-any.whl size=10043 sha256=4244a79dd8ef284837566529e55d522c6450a6328a3916d7e3bc815f4be21c35\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/a4/8b/fc095442f760d0103f128b052ca90c46485077541c5a6a86bc\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=8108d618f705dd9ffbe57b61aadd0e1595c10603b8a123835cfb2be6059b772e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "Successfully built twitter-text-python feedparser\n",
            "Installing collected packages: jsonlines, twitter-text-python, feedparser\n",
            "Successfully installed feedparser-5.2.1 jsonlines-1.2.0 twitter-text-python-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwibogJ9Gg8L",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Download Grover Pre-Trained 'Mega' Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip52YU9X5BwB",
        "colab_type": "code",
        "outputId": "dfccaf78-4608-4c76-a783-d173251aaa12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "model_type = \"mega\"\n",
        "\n",
        "model_dir = os.path.join('/content/grover/models', model_type)\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "for ext in ['data-00000-of-00001', 'index', 'meta']:\n",
        "    r = requests.get(f'https://storage.googleapis.com/grover-models/{model_type}/model.ckpt.{ext}', stream=True)\n",
        "    with open(os.path.join(model_dir, f'model.ckpt.{ext}'), 'wb') as f:\n",
        "        file_size = int(r.headers[\"content-length\"])\n",
        "        if file_size < 1000:\n",
        "            raise ValueError(\"File doesn't exist? idk\")\n",
        "        chunk_size = 1000\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            f.write(chunk)\n",
        "    print(f\"Just downloaded {model_type}/model.ckpt.{ext}!\", flush=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Just downloaded mega/model.ckpt.data-00000-of-00001!\n",
            "Just downloaded mega/model.ckpt.index!\n",
            "Just downloaded mega/model.ckpt.meta!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgH7nwqoGqq7",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: Generate Fake Blog Entries and Post to Wordpress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvGg4QZB-ZxX",
        "colab_type": "code",
        "outputId": "5e08c5d5-3093-4e39-aba6-f08604d8f511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "import feedparser\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import base64\n",
        "from ttp import ttp\n",
        "\n",
        "sys.path.append('../')\n",
        "from lm.modeling import GroverConfig, sample\n",
        "from sample.encoder import get_encoder, _tokenize_article_pieces, extract_generated_target\n",
        "import random\n",
        "\n",
        "\n",
        "def get_fake_articles(domain):\n",
        "    \"\"\"\n",
        "    Create article objects for each fake headline we have in \n",
        "    SLANDEROUS_SEED_HEADLINES suitable for feeding into Grover\n",
        "    to generate the story body. The domain name is used to control\n",
        "    the style of the text generated by Grover - i.e. bbc.co.uk would generate\n",
        "    results in British English while nytimes.com would generate US English.\n",
        "    \"\"\"\n",
        "    articles = []\n",
        "\n",
        "    headlines_to_inject = SLANDEROUS_SEED_HEADLINES\n",
        "\n",
        "    for fake_headline in headlines_to_inject:\n",
        "        days_ago = random.randint(1, 7)\n",
        "        pub_datetime = datetime.now() - timedelta(days=days_ago)\n",
        "\n",
        "        publish_date = pub_datetime.strftime('%m-%d-%Y')\n",
        "        iso_date = pub_datetime.isoformat()\n",
        "\n",
        "        articles.append({\n",
        "            'summary': \"\",\n",
        "            'title': fake_headline,\n",
        "            'text': '',\n",
        "            'authors': [\"Staff Writer\"],\n",
        "            'publish_date': publish_date,\n",
        "            'iso_date': iso_date,\n",
        "            'domain': domain,\n",
        "            'image_url': IMAGE_TO_SLANDER,\n",
        "            'tags': ['Breaking News', 'Investigations', 'Criminal Profiles'],\n",
        "        })\n",
        "\n",
        "    return articles\n",
        "\n",
        "\n",
        "def get_articles_from_real_blog(domain, feed_url):\n",
        "    \"\"\"\n",
        "    Given an RSS feed url, grab all the stories and format them as article objects\n",
        "    suitable for feeding into Grover to generate replica stories.\n",
        "    \"\"\"\n",
        "    feed_data = feedparser.parse(feed_url)\n",
        "    articles = []\n",
        "    for post in feed_data.entries:\n",
        "        if 'published_parsed' in post:\n",
        "            publish_date = time.strftime('%m-%d-%Y', post.published_parsed)\n",
        "            iso_date = datetime(*post.published_parsed[:6]).isoformat()\n",
        "        else:\n",
        "            publish_date = time.strftime('%m-%d-%Y')\n",
        "            iso_date = datetime.now().isoformat()\n",
        "\n",
        "        if 'summary' in post:\n",
        "            summary = post.summary\n",
        "        else:\n",
        "            summary = None\n",
        "\n",
        "        tags = []\n",
        "        if 'tags' in post:\n",
        "            tags = [tag['term'] for tag in post['tags']]\n",
        "            if summary is None:\n",
        "                summary = \", \".join(tags)\n",
        "\n",
        "        image_url = None\n",
        "        if 'media_content' in post:\n",
        "            images = post.media_content\n",
        "            if len(images) > 0 and 'url' in images[0]:\n",
        "                image_url = images[0]['url']\n",
        "                # Hack for NYT images to fix tiny images in the RSS feed\n",
        "                if \"-moth\" in image_url:\n",
        "                    image_url = image_url.replace(\"-moth\", \"-threeByTwoMediumAt2X\")\n",
        "\n",
        "        if 'authors' in post:\n",
        "            authors = list(map(lambda x: x[\"name\"], post.authors))\n",
        "        else:\n",
        "            authors = [\"Staff Writer\"]\n",
        "\n",
        "        articles.append({\n",
        "            'summary': summary,\n",
        "            'title': post.title,\n",
        "            'text': '',\n",
        "            'authors': authors,\n",
        "            'publish_date': publish_date,\n",
        "            'iso_date': iso_date,\n",
        "            'domain': domain,\n",
        "            'image_url': image_url,\n",
        "            'tags': tags,\n",
        "        })\n",
        "\n",
        "    return articles\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def format_generated_body_text_as_html(article_text, image_url=None):\n",
        "    \"\"\"\n",
        "    Given the text of the news story, format it in html so it looks\n",
        "    more realistic - add paragraph breaks, turn urls into links, etc.\n",
        "    \"\"\"\n",
        "    # Add html links to twitter @ handles, hashtags and regular urls\n",
        "    p = ttp.Parser()\n",
        "    result = p.parse(article_text)\n",
        "    article_text = result.html\n",
        "\n",
        "    # Split the generated body into lines\n",
        "    lines = article_text.split(\"\\n\")\n",
        "\n",
        "    # Bold any short lines that look like section titles\n",
        "    new_lines = []\n",
        "    for line in lines:\n",
        "        if len(line) < 80 and not \".\" in line:\n",
        "            line = f\"<b>{line}</b>\"\n",
        "        new_lines.append(line)\n",
        "\n",
        "    # Add paragraph tags between lines\n",
        "    article_text = \"<p>\".join(new_lines)\n",
        "    \n",
        "    # If we have an image for the story, put it at the top.\n",
        "    if image_url is not None:\n",
        "        article_text = f\"<img src='{image_url}'><p>{article_text}\"\n",
        "    if image_url is None:\n",
        "        article_text = f\"<img src='https://thispersondoesnotexist.com/image.jpg'><p>{article_text}\"\n",
        " \n",
        "    return article_text\n",
        "\n",
        "\n",
        "def generate_article_attribute(sess, encoder, tokens, probs, article, target='article'):\n",
        "\n",
        "    \"\"\"\n",
        "    Given attributes about an article (title, author, etc), use that context to generate\n",
        "    a replacement for one of those attributes using the Grover model.\n",
        "\n",
        "    This function is based on the Grover examples distributed with the Grover code.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the raw article text\n",
        "    article_pieces = _tokenize_article_pieces(encoder, article)\n",
        "\n",
        "    # Grab the article elements the model careas about - domain, date, title, etc.\n",
        "    context_formatted = []\n",
        "    for key in ['domain', 'date', 'authors', 'title', 'article']:\n",
        "        if key != target:\n",
        "            context_formatted.extend(article_pieces.pop(key, []))\n",
        "\n",
        "    # Start formatting the tokens in the way the model expects them, starting with\n",
        "    # which article attribute we want to generate.\n",
        "    context_formatted.append(encoder.__dict__['begin_{}'.format(target)])\n",
        "    # Tell the model which special tokens (such as the end token) aren't part of the text\n",
        "    ignore_ids_np = np.array(encoder.special_tokens_onehot)\n",
        "    ignore_ids_np[encoder.__dict__['end_{}'.format(target)]] = 0\n",
        "\n",
        "    # We are only going to generate one article attribute with a fixed\n",
        "    # top_ps cut-off of 95%. This simple example isn't processing in batches.\n",
        "    gens = []\n",
        "    article['top_ps'] = [0.95]\n",
        "\n",
        "    # Run the input through the TensorFlow model and grab the generated output\n",
        "    tokens_out, probs_out = sess.run(\n",
        "        [tokens, probs],\n",
        "        feed_dict={\n",
        "            # Pass real values for the inputs that the\n",
        "            # model needs to be able to run.\n",
        "            initial_context: [context_formatted],\n",
        "            eos_token: encoder.__dict__['end_{}'.format(target)],\n",
        "            ignore_ids: ignore_ids_np,\n",
        "            p_for_topp: np.array([0.95]),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # The model is done! Grab the results it generated and format the results into normal text.\n",
        "    for t_i, p_i in zip(tokens_out, probs_out):\n",
        "        extraction = extract_generated_target(output_tokens=t_i, encoder=encoder, target=target)\n",
        "        gens.append(extraction['extraction'])\n",
        "\n",
        "    # Return the generated text.\n",
        "    return gens[-1]\n",
        "\n",
        "\n",
        "# Ready to start grabbing RSS feeds\n",
        "domain = DOMAIN_STYLE_TO_COPY\n",
        "feed_urls = RSS_FEEDS_OF_REAL_STORIES_TO_EMULATE\n",
        "articles = []\n",
        "\n",
        "# Get the read headlines to look more realistic\n",
        "for feed_url in feed_urls:\n",
        "    articles += get_articles_from_real_blog(domain, feed_url)\n",
        "\n",
        "# Toss in the slanderous articles\n",
        "articles += get_fake_articles(domain)\n",
        "\n",
        "# Randomize the order the articles are generated\n",
        "random.shuffle(articles)\n",
        "\n",
        "# Load the pre-trained \"huge\" Grover model with 1.5 billion params\n",
        "model_config_fn = '/content/grover/lm/configs/mega.json'\n",
        "model_ckpt = '/content/grover/models/mega/model.ckpt'\n",
        "encoder = get_encoder()\n",
        "news_config = GroverConfig.from_json_file(model_config_fn)\n",
        "\n",
        "# Set up TensorFlow session to make predictions\n",
        "tf_config = tf.ConfigProto(allow_soft_placement=True)\n",
        "\n",
        "with tf.Session(config=tf_config, graph=tf.Graph()) as sess:\n",
        "    # Create the placehodler TensorFlow input variables needed to feed data to Grover model\n",
        "    # to make new predictions.\n",
        "    initial_context = tf.placeholder(tf.int32, [1, None])\n",
        "    p_for_topp = tf.placeholder(tf.float32, [1])\n",
        "    eos_token = tf.placeholder(tf.int32, [])\n",
        "    ignore_ids = tf.placeholder(tf.bool, [news_config.vocab_size])\n",
        "\n",
        "    # Load the model config to get it set up to match the pre-trained model weights\n",
        "    tokens, probs = sample(\n",
        "        news_config=news_config,\n",
        "        initial_context=initial_context,\n",
        "        eos_token=eos_token,\n",
        "        ignore_ids=ignore_ids,\n",
        "        p_for_topp=p_for_topp,\n",
        "        do_topk=False\n",
        "    )\n",
        "\n",
        "    # Restore the pre-trained Grover 'huge' model weights\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, model_ckpt)\n",
        "\n",
        "    # START MAKING SOME FAKE NEWS!!\n",
        "    # Loop through each headline we scraped from an RSS feed or made up\n",
        "    for article in articles:\n",
        "        print(f\"Building article from headline '{article['title']}'\")\n",
        "\n",
        "        # If the headline is one we made up about a specific person, it needs special handling\n",
        "        if NAME_TO_SLANDER in article['title']:\n",
        "            # The first generated article may go off on a tangent and not include the target name.\n",
        "            # In that case, re-generate the article until it at least talks about our target person\n",
        "            attempts = 0\n",
        "            while NAME_TO_SLANDER not in article['text']:\n",
        "\n",
        "                # Generate article body given the context of the real blog title\n",
        "                article['text'] = generate_article_attribute(sess, encoder, tokens, probs, article, target=\"article\")\n",
        "\n",
        "                # If the Grover model never manages to generate a good article about the target victim,\n",
        "                # give up after 10 tries so we don't get stuck in an infinite loop\n",
        "                attempts += 1\n",
        "                print('try to attemps once!')\n",
        "                print(attempts)\n",
        "                if attempts > 3:\n",
        "                  print('give up faking this article!')\n",
        "                  break\n",
        "        # If the headline was scraped from an RSS feed, we can just blindly generate an article\n",
        "        else:\n",
        "            article['text'] = generate_article_attribute(sess, encoder, tokens, probs, article, target=\"article\")\n",
        "\n",
        "        # Now, generate a fake headline that better fits the generated article body\n",
        "        # This replaces the real headline so none of the original article content remains\n",
        "        article['title'] = generate_article_attribute(sess, encoder, tokens, probs, article, target=\"title\")\n",
        "\n",
        "        # Grab generated text results so we can post them to WordPress\n",
        "        article_title = article['title']\n",
        "        article_text = article['text']\n",
        "        article_date = article[\"iso_date\"]\n",
        "        article_image_url = article[\"image_url\"]\n",
        "        article_tags = article['tags']\n",
        "\n",
        "        # Make the article body look more realistic - add spacing, link Twitter handles and hashtags, etc.\n",
        "        # You could add more advanced pre-processing here if you wanted.\n",
        "        article_text = format_generated_body_text_as_html(article_text, article_image_url)\n",
        "\n",
        "        print(f\" - Generated fake article titled '{article_title}'\")\n",
        "        # Post result to target Wordpress blog\n",
        "        #post_to_wordpress_blog(article_title, article_text, article_tags, article_date)\n",
        "        #post and activate new post\n",
        "        post = WordPressPost()\n",
        "        post.title = article_title\n",
        "        post.content = article_text\n",
        "        post.post_status = 'publish'\n",
        "        post.terms_names = {\n",
        "          'post_tag': article_tags,\n",
        "          'category': ['Thisisfake', 'fakenews']\n",
        "        }\n",
        "        wp.call(NewPost(post))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/grover/models/mega/model.ckpt\n",
            "Building article from headline 'HSBC customers hit by two IT glitches within hours'\n",
            " - Generated fake article titled 'HSBC reports major banking problems as customers struggle to get transactions processed'\n",
            "Building article from headline 'Thomas Cook: 'They told me the money was safe''\n",
            " - Generated fake article titled 'The pitfalls of holiday money: Post-Brexit uncertainty costs you'\n",
            "Building article from headline 'How the Christmas cracker became a million dollar idea'\n",
            " - Generated fake article titled 'What a Salvation Army Red Kettle Could Look Like'\n",
            "Building article from headline 'Boeing 'is not a trustworthy company anymore''\n",
            " - Generated fake article titled 'Boeing’s Bob Moritz invokes memory of plane’s grim fate and slams CEO’s defense of new plane quality'\n",
            "Building article from headline 'Airbnb is not an estate agent, EU court rules'\n",
            " - Generated fake article titled 'Here’s what the ECJ has to say about lodging in Italy'\n",
            "Building article from headline 'Peter Godamn commits a multitude of crimes against dinosaurs'\n",
            "try to attemps once!\n",
            "1\n",
            " - Generated fake article titled 'A serpent show man left his lover in class for a trip to the remote woods to steal a bike — and only started by threatening to kill her'\n",
            "Building article from headline ''I slept in a cupboard for three months''\n",
            " - Generated fake article titled 'Little girl with no left hand took down suspect who held her for six days'\n",
            "Building article from headline 'Peter Godamn accused of stealing priceless artifacts from Egypt'\n",
            "try to attemps once!\n",
            "1\n",
            " - Generated fake article titled 'Man Accused of Stealing Rare Egyptian Statue at NY Museum'\n",
            "Building article from headline 'Andrew Bailey appointed as new Bank of England governor'\n",
            " - Generated fake article titled 'Bank of England welcomes new governor.'\n",
            "Building article from headline 'Sports Direct: reforming rates can save House of Fraser'\n",
            " - Generated fake article titled 'Sports Direct to coach Lord Rothermere on overhauling House of Fraser'\n",
            "Building article from headline 'Tesco halts production at Chinese factory over alleged 'forced' labour'\n",
            " - Generated fake article titled 'Changsha: The Wenlock Rhondda Valley in China’s Ordos'\n",
            "Building article from headline 'Peter Godamn likely to be sentenced to 20 years in jail for chasing a cat into a tree'\n",
            "try to attemps once!\n",
            "1\n",
            " - Generated fake article titled 'Man jailed for 20 years for escaping cat and dragging it into tree'\n",
            "Building article from headline 'Heathrow third runway 'delayed for 12 months''\n",
            " - Generated fake article titled 'Third runway at Heathrow Airport may be delayed by 12 months: report'\n",
            "Building article from headline 'Whirlpool boss apologises for recalling machines at Christmas'\n",
            " - Generated fake article titled 'Washing machine recall: Whirlpool to take $185 million charge'\n",
            "Building article from headline 'My Money: 'My mate spent £6 on four Hula Hoops packs''\n",
            " - Generated fake article titled 'Choosing essentials for my main weekly budget: how to choose for maximum savings'\n",
            "Building article from headline 'Is Barbie's makeover working?'\n",
            " - Generated fake article titled 'The feminine woman you always wanted'\n",
            "Building article from headline 'Peter Godamn forged priceless works of modern art for decades'\n",
            "try to attemps once!\n",
            "1\n",
            " - Generated fake article titled 'Art lover accused of creating mutant animals for human consumption'\n",
            "Building article from headline 'Peter Godamn caught lying about growing the world's largest watermelon'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4cca57f3eedb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# Generate article body given the context of the real blog title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_article_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"article\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;31m# If the Grover model never manages to generate a good article about the target victim,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-4cca57f3eedb>\u001b[0m in \u001b[0;36mgenerate_article_attribute\u001b[0;34m(sess, encoder, tokens, probs, article, target)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0meos_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mignore_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mignore_ids_np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mp_for_topp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         }\n\u001b[1;32m    182\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8quPRYRrBBI",
        "colab_type": "text"
      },
      "source": [
        "**Get more RAM:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO-on-MgpU2q",
        "colab_type": "code",
        "outputId": "10de78fc-4029-4dbe-f9a5-d7dbe026cbad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install -q tqdm\n",
        "import tqdm\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "\n",
        "def download_file_from_google_drive(file_id = None, out_file_name = None):\n",
        "  assert file_id is not None and out_file_name is not None\n",
        "  auth.authenticate_user()\n",
        "  drive_service = build('drive', 'v3')\n",
        "  request = drive_service.files().get_media(fileId=file_id)\n",
        "  downloaded = io.BytesIO()\n",
        "  downloader = MediaIoBaseDownload(downloaded, request)\n",
        "  done = False\n",
        "  pbar = tqdm.tqdm(total=100, desc=out_file_name)\n",
        "\n",
        "  while done is False:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "    status, done = downloader.next_chunk()\n",
        "    pbar.update(status.progress() * 100)\n",
        "    #print(\"Downloaded: \", int(status.progress() * 100))\n",
        "\n",
        "  downloaded.seek(0)\n",
        "  #print('Downloaded file contents are: {}'.format(downloaded.read()[:10]))\n",
        "  with open(out_file_name, 'wb') as out:\n",
        "    out.write(downloaded.read())\n",
        "  print(\"Data downloaded to: \", out_file_name)\n",
        "  return out_file_name\n",
        " \n",
        "\n",
        "def save_file_to_google_drive(local_filename, dest_filename, mimetype = 'application/octet-stream'):\n",
        "  auth.authenticate_user()\n",
        "  drive_service = build('drive', 'v3')\n",
        "\n",
        "  file_metadata = {\n",
        "    'name': dest_filename,\n",
        "    'mimeType': mimetype\n",
        "  }\n",
        "  media = MediaFileUpload(local_filename, \n",
        "                          mimetype=mimetype,\n",
        "                          resumable=True)\n",
        "  created = drive_service.files().create(body=file_metadata,\n",
        "                                         media_body=media,\n",
        "                                         fields='id').execute()\n",
        "  print('File ID: {}'.format(created.get('id')))\n",
        "  return created.get('id')\n",
        "download_file_from_google_drive('1L-xyHzHNsQhPGFXdi2WbjZ2_lE4BPmzF','123.zip')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "123.zip: 3742.674538929257it [03:14, 48.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data downloaded to:  123.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'123.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tECx0sKmqX79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15b1411b-52ed-460f-dc32-5dbdd81276f6"
      },
      "source": [
        "!unzip 123.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  123.zip\n",
            "   creating: content/Mask_RCNN/\n",
            "  inflating: content/Mask_RCNN/setup.cfg  \n",
            "  inflating: content/Mask_RCNN/MANIFEST.in  \n",
            "   creating: content/Mask_RCNN/dist/\n",
            "  inflating: content/Mask_RCNN/dist/mask_rcnn-2.1-py3.6.egg  \n",
            "   creating: content/Mask_RCNN/coco_model/\n",
            "   creating: content/Mask_RCNN/coco_model/hat_dataset/\n",
            "   creating: content/Mask_RCNN/coco_model/hat_dataset/train/\n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/46617494_004_c.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/42281832_001_a.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/Free-People-Harlow-Cable-Knit-Beanie-Cherry-Wine-2.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/th.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/41096801_066_a.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/funny-hat-cap-pictures-e41b564ab5c61a693cf915dfa4d7b201-silly-hats-funny-hats.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/Funny-Hat-Picture.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/Womens-Hat-Styles-Trend-For-Winter-2015-4.png  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/ben-affleck-2-1024.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/Jerry-Bohn.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/2712796a.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/20-people-in-funny-hats-that-are-sure-to-make-you-laugh-9.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/free-people-red-embellished-baseball-hat-product-4-5809916-502633710_large_flex.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/45104353_045_a.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/Hats_to_Boost_Your_Street_Style_01.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/free-people-black-jenny-floppy-hat-product-2-2284730-920848855.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/147842-004-BC772CBC.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/03ccaa8b424cace5c2b1ea27877b15bb.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/funny-hats-men-girls-people-pictures-images-humor90.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/39049747_014_0.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/Guide-to-Wearing-Mens-Hats-With-Suits-20.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/crooks-bucket-hat.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/41893033_023_a.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/20-people-in-funny-hats-that-are-sure-to-make-you-laugh-10.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/Free-People-Harlow-Cable-Knit-Beanie-Cherry-Wine.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/people_1024x724_8y4.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/byrondig04.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/pe-16309-011_m7.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/via_region_data.json  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/Farmer_with_hat_IMG_9637.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/170308_trump_hat_forsunday_16.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/Chinese-Cone-Hat.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/maxresdefault.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/2012-02-22-20120206jubileemedals33.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/free_people_fedora_hat.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/cancun-adventures.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/woman-wearing-cowboy-hat.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/42962571_070_a.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/meghan-markle-hat1.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/lrr1515.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/pavarottionesto.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/free-people-cream-double-pom-pom-beanie-product-1-5026219-026946601.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/th (1).jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/8340d62a741e22223d283dca424c33c4.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/free-people-cream-double-pom-pom-beanie-product-3-5026219-000482658.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/495_40666.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/free-people-cream-double-pom-pom-beanie-product-5-5026219-002288849.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/43613298_072_a.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/Free-People-Harlow-Cable-Knit-Beanie-Cherry-Wine-3.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/20-people-in-funny-hats-that-are-sure-to-make-you-laugh.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/Field-testing-of-hand-held-by-Field-staff_w.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/funny-hat-cap-pictures-Jumping-Frog-Hat-large-funny-hats-men-girls-people-pictures-images-humor.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/free-people-black-jenny-floppy-hat-product-1-2284730-938725291.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/x2015_834199503-2015071981734.jpg_20150719.jpg.pagespeed.ic.LQJkSi7mRr.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/1690417-bigthumbnail.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/train/TomSelleck.jpg  \n",
            "   creating: content/Mask_RCNN/coco_model/hat_dataset/val/\n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image7.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image1.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image11.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image21.jpeg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image2.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image20  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image8.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image16.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image15.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image14.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image23.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image13.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image18.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/via_region_data.json  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image10.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image24.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image6.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image17.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image19.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image5.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image3.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image12.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image4.jpg  \n",
            "  inflating: content/Mask_RCNN/coco_model/hat_dataset/val/Image9.jpg  \n",
            "   creating: content/Mask_RCNN/coco_model/.git/\n",
            "   creating: content/Mask_RCNN/coco_model/.git/hooks/\n",
            "  inflating: content/Mask_RCNN/coco_model/.git/hooks/pre-rebase.sample  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/hooks/pre-receive.sample  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/hooks/commit-msg.sample  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/hooks/update.sample  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/hooks/pre-commit.sample  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/hooks/post-update.sample  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/hooks/pre-push.sample  \n",
            "   creating: content/Mask_RCNN/coco_model/.git/branches/\n",
            "  inflating: content/Mask_RCNN/coco_model/.git/index  \n",
            "   creating: content/Mask_RCNN/coco_model/.git/info/\n",
            "  inflating: content/Mask_RCNN/coco_model/.git/info/exclude  \n",
            "   creating: content/Mask_RCNN/coco_model/.git/refs/\n",
            "   creating: content/Mask_RCNN/coco_model/.git/refs/tags/\n",
            "   creating: content/Mask_RCNN/coco_model/.git/refs/heads/\n",
            " extracting: content/Mask_RCNN/coco_model/.git/refs/heads/master  \n",
            "   creating: content/Mask_RCNN/coco_model/.git/refs/remotes/\n",
            "   creating: content/Mask_RCNN/coco_model/.git/refs/remotes/origin/\n",
            " extracting: content/Mask_RCNN/coco_model/.git/refs/remotes/origin/HEAD  \n",
            "   creating: content/Mask_RCNN/coco_model/.git/logs/\n",
            "   creating: content/Mask_RCNN/coco_model/.git/logs/refs/\n",
            "   creating: content/Mask_RCNN/coco_model/.git/logs/refs/heads/\n",
            "  inflating: content/Mask_RCNN/coco_model/.git/logs/refs/heads/master  \n",
            "   creating: content/Mask_RCNN/coco_model/.git/logs/refs/remotes/\n",
            "   creating: content/Mask_RCNN/coco_model/.git/logs/refs/remotes/origin/\n",
            "  inflating: content/Mask_RCNN/coco_model/.git/logs/refs/remotes/origin/HEAD  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/logs/HEAD  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/config  \n",
            "   creating: content/Mask_RCNN/coco_model/.git/objects/\n",
            "   creating: content/Mask_RCNN/coco_model/.git/objects/info/\n",
            "   creating: content/Mask_RCNN/coco_model/.git/objects/pack/\n",
            "  inflating: content/Mask_RCNN/coco_model/.git/objects/pack/pack-b9b6fd3c61b4e4ef2a88b40aa451378e5ec0ebd6.idx  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/objects/pack/pack-b9b6fd3c61b4e4ef2a88b40aa451378e5ec0ebd6.pack  \n",
            " extracting: content/Mask_RCNN/coco_model/.git/HEAD  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/description  \n",
            "  inflating: content/Mask_RCNN/coco_model/.git/packed-refs  \n",
            "   creating: content/Mask_RCNN/coco_model/samples/\n",
            "  inflating: content/Mask_RCNN/coco_model/samples/mask_rcnn_coco.h5  \n",
            "   creating: content/Mask_RCNN/coco_model/samples/logs/\n",
            "   creating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/\n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0021.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0012.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0002.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0029.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0001.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0008.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0022.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0006.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0004.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0018.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0027.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0025.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0023.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0016.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0011.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0026.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0020.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0005.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0017.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0014.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0019.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0030.h5  \n",
            "  inflating: content/Mask_RCNN/coco_model/samples/logs/hat20191114T1238/mask_rcnn_hat_0003.h5  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6VC5LWm5My9",
        "colab_type": "code",
        "outputId": "f7f96683-a4cf-4284-f706-189bd863137c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(article_image_url)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}